#!/usr/bin/env python3
"""
Property tests for Task 8: Enhanced Agent Service for Comprehensive Workflows
Tests Properties 6 and 7: Final Approval Gating and Revision Loop Capability
"""
import asyncio
import sys
import os
import uuid
from datetime import datetime
from typing import Dict, Any, List

# Add the backend directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from app.services.revision_targeting_service import (
    get_revision_targeting_service, 
    RevisionType, 
    RevisionScope,
    reset_revision_targeting_service
)
from app.services.langgraph_service import LangGraphService


class MockWebSocketManager:
    """Mock WebSocket manager for testing."""
    
    def __init__(self):
        self.messages = []
        self.connections = {}
    
    async def send_message(self, plan_id: str, message: Dict[str, Any]):
        """Mock send message."""
        self.messages.append({
            "plan_id": plan_id,
            "message": message,
            "timestamp": datetime.utcnow().isoformat()
        })
    
    def get_connection_count(self, plan_id: str) -> int:
        """Mock connection count."""
        return self.connections.get(plan_id, 0)


async def test_property_6_final_approval_gating():
    """
    Property 6: Final Approval Gating
    Validates: Requirements 5.1, 5.2, 5.3, 5.5
    
    Property: For any completed multi-agent workflow, the system MUST:
    1. Present comprehensive results to user before completion
    2. Wait for explicit user approval before marking task complete
    3. Handle approval/rejection appropriately
    4. Support revision requests with intelligent targeting
    """
    print("ğŸ§ª Testing Property 6: Final Approval Gating")
    print("=" * 60)
    
    # Reset service for clean test
    reset_revision_targeting_service()
    service = get_revision_targeting_service()
    
    test_cases = [
        {
            "name": "Final Approval - User Approves",
            "feedback": "Perfect, looks great!",
            "expected_outcome": "completion",
            "should_complete": True
        },
        {
            "name": "Final Approval - User Requests Minor Revision",
            "feedback": "The invoice amount looks wrong, please double-check",
            "expected_outcome": "targeted_revision",
            "should_complete": False
        },
        {
            "name": "Final Approval - User Requests Full Revision",
            "feedback": "This is completely wrong, start over",
            "expected_outcome": "full_revision",
            "should_complete": False
        },
        {
            "name": "Final Approval - User Provides Specific Feedback",
            "feedback": "The CRM data is outdated and the analysis missed key correlations",
            "expected_outcome": "multi_agent_revision",
            "should_complete": False
        }
    ]
    
    # Mock completed workflow state
    completed_state = {
        "plan_id": "test-final-approval",
        "agent_sequence": ["gmail", "invoice", "salesforce", "analysis"],
        "current_step": 4,
        "total_steps": 4,
        "agent_results": {
            "gmail": {"emails_found": 5, "relevant_emails": 3},
            "invoice": {"amount": 1500.00, "vendor": "ACME Corp", "status": "verified"},
            "salesforce": {"account": "University of Chicago", "contact": "John Doe"},
            "analysis": {"summary": "Invoice verification complete", "confidence": 0.95}
        },
        "final_result": "Task completed successfully",
        "awaiting_user_input": True
    }
    
    passed_tests = 0
    total_tests = len(test_cases)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. Testing: {test_case['name']}")
        print(f"   User Feedback: '{test_case['feedback']}'")
        
        try:
            # Parse final approval feedback
            revision_instruction = await service.parse_revision_feedback(
                plan_id=f"final-approval-{i}",
                feedback=test_case['feedback'],
                current_results=completed_state["agent_results"],
                agent_sequence=completed_state["agent_sequence"]
            )
            
            print(f"   ğŸ“Š Revision Type: {revision_instruction.revision_type.value}")
            print(f"   ğŸ¯ Scope: {revision_instruction.revision_scope.value}")
            print(f"   ğŸ“ˆ Confidence: {revision_instruction.confidence_score:.2f}")
            
            # Test Property 6.1: System presents results before completion
            if completed_state["awaiting_user_input"]:
                print(f"   âœ… P6.1: System correctly waits for user input")
            else:
                print(f"   âŒ P6.1: System should wait for user input")
                continue
            
            # Test Property 6.2: Explicit approval required
            if revision_instruction.revision_type == RevisionType.APPROVAL:
                if test_case["should_complete"]:
                    print(f"   âœ… P6.2: Approval correctly detected for completion")
                else:
                    print(f"   âš ï¸  P6.2: Unexpected approval detection")
            else:
                if not test_case["should_complete"]:
                    print(f"   âœ… P6.2: Non-approval correctly detected")
                else:
                    print(f"   âš ï¸  P6.2: Expected approval not detected")
            
            # Test Property 6.3: Appropriate handling of approval/rejection
            if revision_instruction.revision_type == RevisionType.APPROVAL:
                if len(revision_instruction.preserve_results) == len(completed_state["agent_sequence"]):
                    print(f"   âœ… P6.3: Approval preserves all results")
                else:
                    print(f"   âŒ P6.3: Approval should preserve all results")
                    continue
            else:
                if len(revision_instruction.rerun_agents) > 0:
                    print(f"   âœ… P6.3: Revision correctly identifies agents to re-run")
                else:
                    print(f"   âŒ P6.3: Revision should identify agents to re-run")
                    continue
            
            # Test Property 6.4: Intelligent targeting for revisions
            if revision_instruction.revision_type != RevisionType.APPROVAL:
                if revision_instruction.targets:
                    target_agents = [t.agent_name for t in revision_instruction.targets]
                    print(f"   âœ… P6.4: Intelligent targeting identified: {target_agents}")
                    
                    # Verify targeting makes sense
                    feedback_lower = test_case['feedback'].lower()
                    if 'invoice' in feedback_lower and 'invoice' in target_agents:
                        print(f"   âœ… P6.4: Invoice targeting correct")
                    elif 'crm' in feedback_lower and 'salesforce' in target_agents:
                        print(f"   âœ… P6.4: CRM targeting correct")
                    elif 'analysis' in feedback_lower and 'analysis' in target_agents:
                        print(f"   âœ… P6.4: Analysis targeting correct")
                    else:
                        print(f"   âš ï¸  P6.4: Targeting may need refinement")
                else:
                    print(f"   âš ï¸  P6.4: No specific targets identified for revision")
            
            passed_tests += 1
            print(f"   âœ… Property 6 test passed")
            
        except Exception as e:
            print(f"   âŒ Property 6 test failed: {e}")
    
    print(f"\nğŸ“Š Property 6 Results: {passed_tests}/{total_tests} tests passed")
    return passed_tests == total_tests


async def test_property_7_revision_loop_capability():
    """
    Property 7: Revision Loop Capability
    Validates: Requirements 5.4, 10.1, 10.2, 10.3, 10.4, 10.5
    
    Property: The system MUST support iterative revision loops with:
    1. Intelligent parsing of revision requests
    2. Agent-specific targeting vs full workflow restart
    3. Context preservation for unaffected agents
    4. Iteration limits to prevent infinite loops
    5. History tracking for revision patterns
    """
    print("\nğŸ§ª Testing Property 7: Revision Loop Capability")
    print("=" * 60)
    
    # Reset service for clean test
    reset_revision_targeting_service()
    service = get_revision_targeting_service()
    
    # Simulate multiple revision iterations
    plan_id = "test-revision-loop"
    agent_sequence = ["gmail", "invoice", "salesforce", "analysis"]
    current_results = {
        "gmail": {"emails": ["email1", "email2"]},
        "invoice": {"amount": 1000, "vendor": "ACME"},
        "salesforce": {"account": "Test Account"},
        "analysis": {"summary": "Initial analysis"}
    }
    
    revision_scenarios = [
        {
            "iteration": 1,
            "feedback": "The invoice amount is incorrect, should be $1500",
            "expected_targets": ["invoice"],
            "expected_preserve": ["gmail", "salesforce"]
        },
        {
            "iteration": 2,
            "feedback": "Now the CRM data is wrong, update the account info",
            "expected_targets": ["salesforce"],
            "expected_preserve": ["gmail", "invoice"]
        },
        {
            "iteration": 3,
            "feedback": "The analysis needs to include more correlations",
            "expected_targets": ["analysis"],
            "expected_preserve": ["gmail", "invoice", "salesforce"]
        },
        {
            "iteration": 4,
            "feedback": "Everything looks wrong, start completely over",
            "expected_targets": [],  # Full restart
            "expected_preserve": []
        },
        {
            "iteration": 5,
            "feedback": "Just one more small change to the invoice",
            "expected_targets": ["invoice"],
            "should_limit": True  # Should hit iteration limit
        }
    ]
    
    passed_tests = 0
    total_tests = len(revision_scenarios) * 5  # 5 sub-properties per scenario
    
    for scenario in revision_scenarios:
        iteration = scenario["iteration"]
        print(f"\n--- Revision Iteration {iteration} ---")
        print(f"Feedback: '{scenario['feedback']}'")
        
        try:
            # Test Property 7.1: Intelligent parsing
            revision_instruction = await service.parse_revision_feedback(
                plan_id=plan_id,
                feedback=scenario['feedback'],
                current_results=current_results,
                agent_sequence=agent_sequence
            )
            
            print(f"   ğŸ“Š Type: {revision_instruction.revision_type.value}")
            print(f"   ğŸ¯ Scope: {revision_instruction.revision_scope.value}")
            print(f"   ğŸ“ˆ Confidence: {revision_instruction.confidence_score:.2f}")
            print(f"   ğŸ”„ Iteration: {revision_instruction.iteration_count}")
            
            # P7.1: Intelligent parsing
            if revision_instruction.confidence_score > 0.5:
                print(f"   âœ… P7.1: Intelligent parsing successful (confidence: {revision_instruction.confidence_score:.2f})")
                passed_tests += 1
            else:
                print(f"   âŒ P7.1: Low confidence parsing ({revision_instruction.confidence_score:.2f})")
            
            # P7.2: Agent-specific targeting vs full restart
            if scenario.get("expected_targets"):
                target_agents = [t.agent_name for t in revision_instruction.targets]
                if any(expected in target_agents for expected in scenario["expected_targets"]):
                    print(f"   âœ… P7.2: Agent-specific targeting correct: {target_agents}")
                    passed_tests += 1
                else:
                    print(f"   âš ï¸  P7.2: Expected {scenario['expected_targets']}, got {target_agents}")
            else:
                # Full restart expected
                if revision_instruction.revision_type in [RevisionType.FULL_REPLAN, RevisionType.REJECTION]:
                    print(f"   âœ… P7.2: Full restart correctly identified")
                    passed_tests += 1
                else:
                    print(f"   âš ï¸  P7.2: Expected full restart, got targeted revision")
            
            # P7.3: Context preservation
            if scenario.get("expected_preserve"):
                preserved = list(revision_instruction.preserve_results)
                if any(expected in preserved for expected in scenario["expected_preserve"]):
                    print(f"   âœ… P7.3: Context preservation correct: {preserved}")
                    passed_tests += 1
                else:
                    print(f"   âš ï¸  P7.3: Expected to preserve {scenario['expected_preserve']}, got {preserved}")
            else:
                # Full restart - nothing should be preserved
                if len(revision_instruction.preserve_results) == 0:
                    print(f"   âœ… P7.3: Full restart preserves nothing (correct)")
                    passed_tests += 1
                else:
                    print(f"   âš ï¸  P7.3: Full restart should preserve nothing")
            
            # P7.4: Iteration limits
            if scenario.get("should_limit"):
                should_limit = service.should_limit_revisions(plan_id, max_iterations=4)
                if should_limit:
                    print(f"   âœ… P7.4: Iteration limit correctly enforced")
                    passed_tests += 1
                else:
                    print(f"   âŒ P7.4: Iteration limit should be enforced")
            else:
                should_limit = service.should_limit_revisions(plan_id, max_iterations=10)
                if not should_limit:
                    print(f"   âœ… P7.4: Iteration limit not yet reached")
                    passed_tests += 1
                else:
                    print(f"   âš ï¸  P7.4: Unexpected iteration limit")
            
            # P7.5: History tracking
            history = service.get_revision_history(plan_id)
            if len(history) == iteration:
                print(f"   âœ… P7.5: Revision history correctly tracked ({len(history)} entries)")
                passed_tests += 1
            else:
                print(f"   âŒ P7.5: Expected {iteration} history entries, got {len(history)}")
            
        except Exception as e:
            print(f"   âŒ Revision iteration {iteration} failed: {e}")
    
    # Test revision statistics
    print(f"\n--- Revision Statistics ---")
    stats = service.get_revision_stats()
    print(f"Total revisions: {stats['total_revisions']}")
    print(f"Plans with revisions: {stats['plans_with_revisions']}")
    print(f"Average iterations: {stats['average_iterations']:.1f}")
    print(f"Revision types: {stats['revision_types']}")
    
    print(f"\nğŸ“Š Property 7 Results: {passed_tests}/{total_tests} sub-tests passed")
    return passed_tests >= (total_tests * 0.8)  # 80% pass rate acceptable


async def test_integration_revision_with_langgraph():
    """
    Integration test: Revision targeting with LangGraph state management
    Tests the complete revision flow including state updates and execution resumption.
    """
    print("\nğŸ§ª Testing Integration: Revision Targeting + LangGraph")
    print("=" * 60)
    
    # Mock plan state
    plan_id = "test-integration-revision"
    mock_state = {
        "plan_id": plan_id,
        "agent_sequence": ["gmail", "invoice", "salesforce", "analysis"],
        "current_step": 4,
        "total_steps": 4,
        "agent_results": {
            "gmail": {"emails_found": 3},
            "invoice": {"amount": 1000, "verified": True},
            "salesforce": {"account": "Test Corp"},
            "analysis": {"summary": "Complete"}
        },
        "awaiting_user_input": True
    }
    
    # Test revision instruction creation
    service = get_revision_targeting_service()
    
    test_feedback = "The invoice amount is wrong and the analysis is incomplete"
    
    try:
        revision_instruction = await service.parse_revision_feedback(
            plan_id=plan_id,
            feedback=test_feedback,
            current_results=mock_state["agent_results"],
            agent_sequence=mock_state["agent_sequence"]
        )
        
        print(f"âœ… Revision instruction created successfully")
        print(f"   Type: {revision_instruction.revision_type.value}")
        print(f"   Targets: {[t.agent_name for t in revision_instruction.targets]}")
        print(f"   Preserve: {revision_instruction.preserve_results}")
        print(f"   Re-run: {revision_instruction.rerun_agents}")
        
        # Test state update preparation (simulating LangGraphService.resume_execution_with_revision)
        updates = {
            "result_approved": False,
            "hitl_feedback": test_feedback,
            "awaiting_user_input": False,
            "revision_instruction": {
                "type": revision_instruction.revision_type.value,
                "scope": revision_instruction.revision_scope.value,
                "preserve_results": list(revision_instruction.preserve_results),
                "rerun_agents": list(revision_instruction.rerun_agents),
                "targets": [
                    {
                        "agent_name": t.agent_name,
                        "revision_reason": t.revision_reason,
                        "preserve_context": t.preserve_context,
                        "priority": t.priority
                    }
                    for t in revision_instruction.targets
                ],
                "iteration_count": revision_instruction.iteration_count,
                "confidence_score": revision_instruction.confidence_score
            }
        }
        
        print(f"âœ… State updates prepared for LangGraph integration")
        print(f"   Preserved results: {updates['revision_instruction']['preserve_results']}")
        print(f"   Agents to re-run: {updates['revision_instruction']['rerun_agents']}")
        
        # Verify integration readiness
        if updates["revision_instruction"]["preserve_results"]:
            print(f"âœ… Integration ready: Context preservation configured")
        
        if updates["revision_instruction"]["rerun_agents"]:
            print(f"âœ… Integration ready: Agent re-execution configured")
        
        if updates["revision_instruction"]["confidence_score"] > 0.6:
            print(f"âœ… Integration ready: High confidence revision ({updates['revision_instruction']['confidence_score']:.2f})")
        
        return True
        
    except Exception as e:
        print(f"âŒ Integration test failed: {e}")
        return False


async def run_all_task8_property_tests():
    """Run all Task 8 property tests."""
    print("ğŸš€ Running Task 8 Property Tests")
    print("=" * 80)
    
    results = []
    
    # Test Property 6: Final Approval Gating
    try:
        result_p6 = await test_property_6_final_approval_gating()
        results.append(("Property 6 - Final Approval Gating", result_p6))
    except Exception as e:
        print(f"âŒ Property 6 test suite failed: {e}")
        results.append(("Property 6 - Final Approval Gating", False))
    
    # Test Property 7: Revision Loop Capability
    try:
        result_p7 = await test_property_7_revision_loop_capability()
        results.append(("Property 7 - Revision Loop Capability", result_p7))
    except Exception as e:
        print(f"âŒ Property 7 test suite failed: {e}")
        results.append(("Property 7 - Revision Loop Capability", False))
    
    # Test Integration
    try:
        result_integration = await test_integration_revision_with_langgraph()
        results.append(("Integration - Revision + LangGraph", result_integration))
    except Exception as e:
        print(f"âŒ Integration test failed: {e}")
        results.append(("Integration - Revision + LangGraph", False))
    
    # Summary
    print(f"\nğŸ Task 8 Property Tests Summary")
    print("=" * 80)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"{status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š Overall Results: {passed}/{total} test suites passed")
    
    if passed == total:
        print("ğŸ‰ All Task 8 property tests PASSED!")
        print("âœ… Requirements 5.1, 5.2, 5.3, 5.4, 5.5, 10.1, 10.2, 10.3, 10.4, 10.5 validated")
    else:
        print("âš ï¸  Some Task 8 property tests failed - review implementation")
    
    return passed == total


if __name__ == "__main__":
    asyncio.run(run_all_task8_property_tests())